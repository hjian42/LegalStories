{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Native Speakers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Load immediate RCT results from the control and treatment groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_story - native (160, 20)\n",
      "df_def - native (165, 19)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_story = pd.read_csv(\"./rct_results/10_concepts_native_story_scores.tsv\", sep=\"\\t\")\n",
    "df_def = pd.read_csv(\"./rct_results/10_concepts_native_definition_scores.tsv\", sep=\"\\t\")\n",
    "print(\"df_story - native\", df_story.shape)\n",
    "print(\"df_def - native\", df_def.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sanity check -- no intersection of participants in the same batch\n",
    "story_group_tuple_set = set(tuple(x) for x in df_story[['batch_id', 'PROLIFIC_PID']].to_numpy())\n",
    "def_group_tuple_set = set(tuple(x) for x in df_def[['batch_id', 'PROLIFIC_PID']].to_numpy())\n",
    "assert len(story_group_tuple_set.intersection(def_group_tuple_set)) == 0, \"there is some overlap between two groups\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RCT results for the control group among the non-native speakers\n",
      "q1_pred    93.33\n",
      "q2_pred    78.79\n",
      "q3_pred    77.58\n",
      "dtype: float64\n",
      "q1_pred    0.1203\n",
      "q2_pred    0.2127\n",
      "q3_pred    0.2258\n",
      "dtype: float64\n",
      "Familiarity Score: mean: 1.8848484848484848, std: 1.226764834273666\n",
      "Relevance Score: mean: 2.6303030303030304, std: 1.3076894650034483\n",
      "Interest Score: mean: 3.6666666666666665, std: 1.1225696422164713\n",
      "perceived_difficulty: mean: 3.096969696969697, std: 1.2058002222198316\n"
     ]
    }
   ],
   "source": [
    "print(\"RCT results for the control group among the non-native speakers\")\n",
    "\n",
    "df_def['q1_pred'] = (df_def['conceptQ'] == df_def['concept_question_answer'])\n",
    "df_def['q2_pred'] = (df_def['predictionQ'] == df_def['prediction_question_answer'])\n",
    "df_def['q3_pred'] = (df_def['limitationQ'] == df_def['limitation_question_answer'])\n",
    "\n",
    "print((df_def[['q1_pred', 'q2_pred', 'q3_pred']].sum()*100 / df_def.shape[0]).round(2))\n",
    "print(df_def[['PROLIFIC_PID', 'q1_pred', 'q2_pred', 'q3_pred']].groupby(\"PROLIFIC_PID\").mean().reset_index()[['q1_pred', 'q2_pred', 'q3_pred']].astype(float).std().round(4))\n",
    "print(\"Familiarity Score: mean: {}, std: {}\".format(df_def['familiarity_concept'].astype(float).mean(), df_def['familiarity_concept'].astype(float).std()))\n",
    "print(\"Relevance Score: mean: {}, std: {}\".format(df_def['relevance'].astype(float).mean(), df_def['relevance'].astype(float).std()))\n",
    "print(\"Interest Score: mean: {}, std: {}\".format(df_def['law_interest'].astype(float).mean(), df_def['law_interest'].astype(float).std()))\n",
    "print(\"perceived_difficulty: mean: {}, std: {}\".format(df_def['perceived_difficulty'].astype(float).mean(), df_def['perceived_difficulty'].astype(float).std()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RCT results for the treatment group among the native speakers\n",
      "q1_pred    90.62\n",
      "q2_pred    74.38\n",
      "q3_pred    84.38\n",
      "dtype: float64\n",
      "q1_pred    0.1900\n",
      "q2_pred    0.2651\n",
      "q3_pred    0.2422\n",
      "dtype: float64\n",
      "Familiarity Score: mean: 1.74375, std: 1.1505911797389556\n",
      "Relevance Score: mean: 3.20625, std: 1.3274881704811141\n",
      "Interest Score: mean: 3.78125, std: 0.9947780321927768\n",
      "perceived_difficulty: mean: 3.14375, std: 1.2226729919659707\n"
     ]
    }
   ],
   "source": [
    "print(\"RCT results for the treatment group among the native speakers\")\n",
    "\n",
    "df_story['q1_pred'] = (df_story['conceptQ'] == df_story['concept_question_answer'])\n",
    "df_story['q2_pred'] = (df_story['predictionQ'] == df_story['prediction_question_answer'])\n",
    "df_story['q3_pred'] = (df_story['limitationQ'] == df_story['limitation_question_answer'])\n",
    "\n",
    "print((df_story[['q1_pred', 'q2_pred', 'q3_pred']].sum()*100 / df_story.shape[0]).round(2))\n",
    "print(df_story[['PROLIFIC_PID', 'q1_pred', 'q2_pred', 'q3_pred']].groupby(\"PROLIFIC_PID\").mean().reset_index()[['q1_pred', 'q2_pred', 'q3_pred']].astype(float).std().round(4))\n",
    "# print(df_story[['q1_pred', 'q2_pred', 'q3_pred', 'q_id']].groupby(by=\"q_id\").sum() / 16)\n",
    "print(\"Familiarity Score: mean: {}, std: {}\".format(df_story['familiarity_concept'].astype(float).mean(), df_story['familiarity_concept'].astype(float).std()))\n",
    "print(\"Relevance Score: mean: {}, std: {}\".format(df_story['relevance'].astype(float).mean(), df_story['relevance'].astype(float).std()))\n",
    "print(\"Interest Score: mean: {}, std: {}\".format(df_story['law_interest'].astype(float).mean(), df_story['law_interest'].astype(float).std()))\n",
    "print(\"perceived_difficulty: mean: {}, std: {}\".format(df_story['perceived_difficulty'].astype(float).mean(), df_story['perceived_difficulty'].astype(float).std()))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Load follow-up RCT results from the control and treatment groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(165, 19) (160, 20)\n",
      "(100, 11) (105, 11)\n",
      "(100, 19) (105, 20)\n"
     ]
    }
   ],
   "source": [
    "def compute_group_accuracy(df_):\n",
    "    df_['conceptQ'] = df_['conceptQ'].astype(float)\n",
    "    df_['concept_question_answer'] = df_['concept_question_answer'].astype(float)\n",
    "    df_['predictionQ'] = df_['predictionQ'].astype(float)\n",
    "    df_['prediction_question_answer'] = df_['prediction_question_answer'].astype(float)\n",
    "    df_['limitationQ'] = df_['limitationQ'].astype(float)\n",
    "    df_['limitation_question_answer'] = df_['limitation_question_answer'].astype(float)\n",
    "    \n",
    "    df_['q1_pred'] = (df_['conceptQ'] == df_['concept_question_answer'])\n",
    "    df_['q2_pred'] = (df_['predictionQ'] == df_['prediction_question_answer'])\n",
    "    df_['q3_pred'] = (df_['limitationQ'] == df_['limitation_question_answer'])\n",
    "\n",
    "    # num_ppl = df_.shape[0] / 10\n",
    "    # print(\"num_ppl\", num_ppl)\n",
    "\n",
    "    # print((df_[['q1_pred', 'q2_pred', 'q3_pred']].sum()*100 / df_.shape[0]).round(2))\n",
    "    # print(df_[['PROLIFIC_PID', 'q1_pred', 'q2_pred', 'q3_pred']].groupby(\"PROLIFIC_PID\").mean().reset_index()[['q1_pred', 'q2_pred', 'q3_pred']].astype(float).std().round(4))\n",
    "\n",
    "    # print(\"Familiarity Score: mean: {}, std: {}\".format(df_['familiarity_concept'].astype(float).mean(), df_['familiarity_concept'].astype(float).std()))\n",
    "    return df_\n",
    "\n",
    "def compute_group_retention_accuracy(df0, df1):\n",
    "    df0_ = df0.sort_values(by=['concept', 'PROLIFIC_PID'])\n",
    "    df1_ = df1.sort_values(by=['concept', 'PROLIFIC_PID'])\n",
    "\n",
    "    for questionPrediction in ['q1_pred', 'q2_pred', 'q3_pred']:\n",
    "        df0_tmp = df0_[df0_[questionPrediction]].sort_values(by=['concept', 'PROLIFIC_PID'])\n",
    "        df1_tmp = df1_.merge(df0_tmp[['PROLIFIC_PID', 'q_id']], on=['PROLIFIC_PID', 'q_id']).sort_values(by=['concept', 'PROLIFIC_PID'])\n",
    "        print((df1_tmp[[questionPrediction]].sum()*100 / df1_tmp.shape[0]).round(2))\n",
    "\n",
    "\n",
    "df_def = pd.read_csv(\"./rct_results/10_concepts_native_definition_scores.tsv\", sep=\"\\t\")\n",
    "df_story = pd.read_csv(\"./rct_results/10_concepts_native_story_scores.tsv\", sep=\"\\t\")\n",
    "\n",
    "df_def_followup = pd.read_csv(\"./rct_results/10_concepts_native_definition_scores_followup.tsv\", sep=\"\\t\")\n",
    "df_story_followup = pd.read_csv(\"./rct_results/10_concepts_native_story_scores_followup.tsv\", sep=\"\\t\")\n",
    "\n",
    "df_def_both = df_def.merge(df_def_followup[['PROLIFIC_PID', 'q_id']], on=['PROLIFIC_PID', 'q_id'])\n",
    "df_story_both = df_story.merge(df_story_followup[['PROLIFIC_PID', 'q_id']], on=['PROLIFIC_PID', 'q_id'])\n",
    "\n",
    "print(df_def.shape, df_story.shape)\n",
    "print(df_def_followup.shape, df_story_followup.shape)\n",
    "print(df_def_both.shape, df_story_both.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rentention Scores for the Control Group -- Native Speakers\n",
      "q1_pred    92.55\n",
      "dtype: float64\n",
      "q2_pred    88.89\n",
      "dtype: float64\n",
      "q3_pred    91.03\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "df_def_both = compute_group_accuracy(df_def_both)\n",
    "df_def_followup = compute_group_accuracy(df_def_followup)\n",
    "\n",
    "print(\"Rentention Scores for the Control Group -- Native Speakers\")\n",
    "compute_group_retention_accuracy(df_def_both, df_def_followup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rentention Scores for the Treatment Group -- Native Speakers\n",
      "q1_pred    91.58\n",
      "dtype: float64\n",
      "q2_pred    86.84\n",
      "dtype: float64\n",
      "q3_pred    91.01\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "df_story_both = compute_group_accuracy(df_story_both)\n",
    "df_story_followup = compute_group_accuracy(df_story_followup)\n",
    "\n",
    "print(\"Rentention Scores for the Treatment Group -- Native Speakers\")\n",
    "compute_group_retention_accuracy(df_story_both, df_story_followup)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Non-native Speakers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Load immediate RCT results from the control and treatment groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_story - non-native (170, 20)\n",
      "df_def - non-native (185, 19)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_story = pd.read_csv(\"./rct_results/10_concepts_non_native_story_scores.tsv\", sep=\"\\t\")\n",
    "df_def = pd.read_csv(\"./rct_results/10_concepts_non_native_definition_scores.tsv\", sep=\"\\t\")\n",
    "print(\"df_story - non-native\", df_story.shape)\n",
    "print(\"df_def - non-native\", df_def.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RCT results for the control group among the non-native speakers\n",
      "q1_pred    89.19\n",
      "q2_pred    71.89\n",
      "q3_pred    68.65\n",
      "dtype: float64\n",
      "q1_pred    0.1455\n",
      "q2_pred    0.2434\n",
      "q3_pred    0.2627\n",
      "dtype: float64\n",
      "Familiarity Score: mean: 1.8108108108108107, std: 0.9901097281592252\n",
      "Relevance Score: mean: 2.4702702702702704, std: 1.1704850501582447\n",
      "Interest Score: mean: 3.8378378378378377, std: 0.9182187835315079\n",
      "perceived_difficulty: mean: 3.2, std: 1.1315629821661257\n"
     ]
    }
   ],
   "source": [
    "print(\"RCT results for the control group among the non-native speakers\")\n",
    "\n",
    "df_def['q1_pred'] = (df_def['conceptQ'] == df_def['concept_question_answer'])\n",
    "df_def['q2_pred'] = (df_def['predictionQ'] == df_def['prediction_question_answer'])\n",
    "df_def['q3_pred'] = (df_def['limitationQ'] == df_def['limitation_question_answer'])\n",
    "\n",
    "print((df_def[['q1_pred', 'q2_pred', 'q3_pred']].sum()*100 / df_def.shape[0]).round(2))\n",
    "print(df_def[['PROLIFIC_PID', 'q1_pred', 'q2_pred', 'q3_pred']].groupby(\"PROLIFIC_PID\").mean().reset_index()[['q1_pred', 'q2_pred', 'q3_pred']].astype(float).std().round(4))\n",
    "print(\"Familiarity Score: mean: {}, std: {}\".format(df_def['familiarity_concept'].astype(float).mean(), df_def['familiarity_concept'].astype(float).std()))\n",
    "print(\"Relevance Score: mean: {}, std: {}\".format(df_def['relevance'].astype(float).mean(), df_def['relevance'].astype(float).std()))\n",
    "print(\"Interest Score: mean: {}, std: {}\".format(df_def['law_interest'].astype(float).mean(), df_def['law_interest'].astype(float).std()))\n",
    "print(\"perceived_difficulty: mean: {}, std: {}\".format(df_def['perceived_difficulty'].astype(float).mean(), df_def['perceived_difficulty'].astype(float).std()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RCT results for the treatment group among the non-native speakers\n",
      "q1_pred    91.18\n",
      "q2_pred    81.76\n",
      "q3_pred    84.71\n",
      "dtype: float64\n",
      "q1_pred    0.1235\n",
      "q2_pred    0.1819\n",
      "q3_pred    0.1844\n",
      "dtype: float64\n",
      "Familiarity Score: mean: 1.6294117647058823, std: 1.0310359619772527\n",
      "Relevance Score: mean: 3.1882352941176473, std: 1.171597984720104\n",
      "Interest Score: mean: 4.029411764705882, std: 1.2036751935046288\n",
      "perceived_difficulty: mean: 3.052941176470588, std: 1.2929566351678787\n"
     ]
    }
   ],
   "source": [
    "print(\"RCT results for the treatment group among the non-native speakers\")\n",
    "\n",
    "df_story['q1_pred'] = (df_story['conceptQ'] == df_story['concept_question_answer'])\n",
    "df_story['q2_pred'] = (df_story['predictionQ'] == df_story['prediction_question_answer'])\n",
    "df_story['q3_pred'] = (df_story['limitationQ'] == df_story['limitation_question_answer'])\n",
    "\n",
    "print((df_story[['q1_pred', 'q2_pred', 'q3_pred']].sum()*100 / df_story.shape[0]).round(2))\n",
    "print(df_story[['PROLIFIC_PID', 'q1_pred', 'q2_pred', 'q3_pred']].groupby(\"PROLIFIC_PID\").mean().reset_index()[['q1_pred', 'q2_pred', 'q3_pred']].astype(float).std().round(4))\n",
    "# print(df_story[['q1_pred', 'q2_pred', 'q3_pred', 'q_id']].groupby(by=\"q_id\").sum() / 16)\n",
    "print(\"Familiarity Score: mean: {}, std: {}\".format(df_story['familiarity_concept'].astype(float).mean(), df_story['familiarity_concept'].astype(float).std()))\n",
    "print(\"Relevance Score: mean: {}, std: {}\".format(df_story['relevance'].astype(float).mean(), df_story['relevance'].astype(float).std()))\n",
    "print(\"Interest Score: mean: {}, std: {}\".format(df_story['law_interest'].astype(float).mean(), df_story['law_interest'].astype(float).std()))\n",
    "print(\"perceived_difficulty: mean: {}, std: {}\".format(df_story['perceived_difficulty'].astype(float).mean(), df_story['perceived_difficulty'].astype(float).std()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sanity check -- no intersection of participants in the same batch\n",
    "story_group_tuple_set = set(tuple(x) for x in df_story[['batch_id', 'PROLIFIC_PID']].to_numpy())\n",
    "def_group_tuple_set = set(tuple(x) for x in df_def[['batch_id', 'PROLIFIC_PID']].to_numpy())\n",
    "assert len(story_group_tuple_set.intersection(def_group_tuple_set)) == 0, \"there is some overlap between two groups\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Load follow-up RCT results from the control and treatment groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(185, 19) (170, 20)\n",
      "(125, 11) (150, 11)\n",
      "(125, 19) (150, 20)\n"
     ]
    }
   ],
   "source": [
    "df_def = pd.read_csv(\"./rct_results/10_concepts_non_native_definition_scores.tsv\", sep=\"\\t\")\n",
    "df_story = pd.read_csv(\"./rct_results/10_concepts_non_native_story_scores.tsv\", sep=\"\\t\")\n",
    "\n",
    "df_def_followup = pd.read_csv(\"./rct_results/10_concepts_non_native_definition_scores_followup.tsv\", sep=\"\\t\")\n",
    "df_story_followup = pd.read_csv(\"./rct_results/10_concepts_non_native_story_scores_followup.tsv\", sep=\"\\t\")\n",
    "\n",
    "df_def_both = df_def.merge(df_def_followup[['PROLIFIC_PID', 'q_id']], on=['PROLIFIC_PID', 'q_id'])\n",
    "df_story_both = df_story.merge(df_story_followup[['PROLIFIC_PID', 'q_id']], on=['PROLIFIC_PID', 'q_id'])\n",
    "\n",
    "print(df_def.shape, df_story.shape)\n",
    "print(df_def_followup.shape, df_story_followup.shape)\n",
    "print(df_def_both.shape, df_story_both.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rentention Scores for the Control Group -- Non-native Speakers\n",
      "q1_pred    86.32\n",
      "dtype: float64\n",
      "q2_pred    82.8\n",
      "dtype: float64\n",
      "q3_pred    91.01\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "df_def_both = compute_group_accuracy(df_def_both)\n",
    "df_def_followup = compute_group_accuracy(df_def_followup)\n",
    "\n",
    "print(\"Rentention Scores for the Control Group -- Non-native Speakers\")\n",
    "compute_group_retention_accuracy(df_def_both, df_def_followup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rentention Scores for the Treatment Group -- Native Speakers\n",
      "q1_pred    98.56\n",
      "dtype: float64\n",
      "q2_pred    89.6\n",
      "dtype: float64\n",
      "q3_pred    92.31\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "df_story_both = compute_group_accuracy(df_story_both)\n",
    "df_story_followup = compute_group_accuracy(df_story_followup)\n",
    "\n",
    "print(\"Rentention Scores for the Treatment Group -- Native Speakers\")\n",
    "compute_group_retention_accuracy(df_story_both, df_story_followup)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
