{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# libs\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.formula.api as smf\n",
    "from scipy.stats import chi2_contingency\n",
    "from scipy.stats import mannwhitneyu\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load RCT Results\n",
    "1. Load RCT Result\n",
    "2. Prepare data for accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIR = os.path.join(os.getcwd(), \"rct_results\")\n",
    "\n",
    "df_dict = {}\n",
    "pre_fix = \"10_concepts\"\n",
    "target_questions = ['concept', 'prediction', 'limitation']\n",
    "for group in [\"non_native\", \"native\"]:\n",
    "    if group not in df_dict.keys():\n",
    "        df_dict[group] = dict()\n",
    "    for condition in ['definition', 'story']:\n",
    "        if condition not in df_dict.keys():\n",
    "            df_dict[group][condition] = dict()\n",
    "        for study in [\"scores\", \"scores_followup\"]:\n",
    "            file_path = os.path.join(INPUT_DIR, '_'.join([pre_fix, group, condition, study + \".tsv\"]))\n",
    "            study_df = pd.read_csv(file_path, sep='\\t')\n",
    "            df_dict[group][condition][study] = study_df.copy()\n",
    "\n",
    "for group in df_dict.keys():\n",
    "    for condition in df_dict[group].keys():\n",
    "        for study in [\"scores\", \"scores_followup\"]:\n",
    "            raw_df = df_dict[group][condition][study]\n",
    "            for q in target_questions:\n",
    "                # definition\n",
    "                real_answer = np.array(raw_df[q + \"_question_answer\"].values)\n",
    "                user_answer = np.array(raw_df[q + \"Q\"].values)\n",
    "                correctness = np.multiply((real_answer == user_answer), 1)\n",
    "                raw_df[q + \"_rst\"] = correctness\n",
    "            df_dict[group][condition][study] = raw_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statistical Analysis\n",
    "1. Chi-Squared for the question accuracy in the test and the follow-up test\n",
    "2. Mann-Whitney test for the relevance score and interest in law"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chi Squared for Accuracy\n",
      "For Group  native\n",
      "*************\n",
      "Question:  concept_rst\n",
      "0.48339200428194906 0.4868898821253125\n",
      "*************\n",
      "Question:  prediction_rst\n",
      "0.6537854593213362 0.41876252154968585\n",
      "*************\n",
      "Question:  limitation_rst\n",
      "2.0120481800016 0.1560545129081714\n",
      "For Group  non_native\n",
      "*************\n",
      "Question:  concept_rst\n",
      "0.20182537119577626 0.6532514758125724\n",
      "*************\n",
      "Question:  prediction_rst\n",
      "4.2850363890097 0.0384492595157595\n",
      "*************\n",
      "Question:  limitation_rst\n",
      "11.772340686798307 0.0006011733562701144\n",
      "\n",
      "\n",
      "Mann-Whitney U Test\n",
      "For Group  native\n",
      "Metric:  relevance\n",
      "Story Condition MD(STD):  3.20625 1.3233332677371943\n",
      "Definition Condition MD(STD):  2.6303030303030304 1.3037207473076875\n",
      "16421.0 0.00010137547692223972\n",
      "\n",
      "\n",
      "Metric:  law_interest\n",
      "Story Condition MD(STD):  3.78125 0.9916644782889019\n",
      "Definition Condition MD(STD):  3.6666666666666665 1.119162746219357\n",
      "13800.0 0.45934243459962987\n",
      "\n",
      "\n",
      "For Group  non_native\n",
      "Metric:  relevance\n",
      "Story Condition MD(STD):  3.1882352941176473 1.1681470258697946\n",
      "Definition Condition MD(STD):  2.4702702702702704 1.167317290492911\n",
      "21077.5 1.246935239355139e-08\n",
      "\n",
      "\n",
      "Metric:  law_interest\n",
      "Story Condition MD(STD):  4.029411764705882 1.2001297507707729\n",
      "Definition Condition MD(STD):  3.8378378378378377 0.9157337484376996\n",
      "18662.5 0.0013574027581332746\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Mann Whitney\n",
    "def mannwhitneyu_test(data1, data2):\n",
    "    u1, p = mannwhitneyu(data1, data2, method=\"asymptotic\")\n",
    "    threshold = 0.05\n",
    "    if p < threshold:\n",
    "        # Statistically different\n",
    "        n = True\n",
    "    else:\n",
    "        n = False\n",
    "    return n, u1, p\n",
    "\n",
    "# Chi-Squared test of accuracy\n",
    "print(\"Chi Squared for Accuracy\")\n",
    "for group in [\"native\", \"non_native\"]:\n",
    "    print(\"For Group \", group)\n",
    "    for question in [q + \"_rst\" for q in target_questions]:\n",
    "        print(\"*************\")\n",
    "        print(\"Question: \", question)\n",
    "        contingency_table = []\n",
    "        for condition in [\"story\", \"definition\"]:  \n",
    "            data = df_dict[group][condition][\"scores\"]\n",
    "            incorrect = data[question].value_counts()[0]\n",
    "            correct = data[question].value_counts()[1]\n",
    "            contingency_table.append([correct, incorrect])\n",
    "        stat, p, dof, expected = chi2_contingency(contingency_table)\n",
    "        print(stat, p)\n",
    "\n",
    "# Mann-Whitney\n",
    "print(\"\\n\")\n",
    "print(\"Mann-Whitney U Test\")\n",
    "for group in [\"native\", \"non_native\"]:\n",
    "    print(\"For Group \", group)\n",
    "    for metric in [\"relevance\", \"law_interest\"]:  \n",
    "        data1 = df_dict[group]['story']['scores'][metric]\n",
    "        data2 = df_dict[group]['definition']['scores'][metric]\n",
    "        print(\"Metric: \", metric)\n",
    "        print(\"Story Condition MD(STD): \", np.mean(data1), np.std(data1))\n",
    "        print(\"Definition Condition MD(STD): \", np.mean(data2), np.std(data2))\n",
    "        n, u, p = mannwhitneyu_test(data1, data2)\n",
    "        print(u, p)\n",
    "        print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Followup Assessment result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question:  concept_rst\n",
      "86.32\n",
      "Question:  prediction_rst\n",
      "82.8\n",
      "Question:  limitation_rst\n",
      "91.01\n",
      "\n",
      "\n",
      "Question:  concept_rst\n",
      "98.56\n",
      "Question:  prediction_rst\n",
      "89.60000000000001\n",
      "Question:  limitation_rst\n",
      "92.31\n",
      "\n",
      "\n",
      "Question:  concept_rst\n",
      "92.55\n",
      "Question:  prediction_rst\n",
      "88.89\n",
      "Question:  limitation_rst\n",
      "91.03\n",
      "\n",
      "\n",
      "Question:  concept_rst\n",
      "91.58\n",
      "Question:  prediction_rst\n",
      "86.83999999999999\n",
      "Question:  limitation_rst\n",
      "91.01\n",
      "\n",
      "\n",
      "Total Responding Rate:  0.71\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/zl/b_kmp9gj7nx2fx9721_dt5mc0000gn/T/ipykernel_19842/3778341594.py:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  retention_population[question + \"_y\"] = retention_population[question + \"_y\"].replace({True: 1, False: 0})\n",
      "/var/folders/zl/b_kmp9gj7nx2fx9721_dt5mc0000gn/T/ipykernel_19842/3778341594.py:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  retention_population[question + \"_y\"] = retention_population[question + \"_y\"].replace({True: 1, False: 0})\n",
      "/var/folders/zl/b_kmp9gj7nx2fx9721_dt5mc0000gn/T/ipykernel_19842/3778341594.py:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  retention_population[question + \"_y\"] = retention_population[question + \"_y\"].replace({True: 1, False: 0})\n",
      "/var/folders/zl/b_kmp9gj7nx2fx9721_dt5mc0000gn/T/ipykernel_19842/3778341594.py:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  retention_population[question + \"_y\"] = retention_population[question + \"_y\"].replace({True: 1, False: 0})\n"
     ]
    }
   ],
   "source": [
    "total_respondant = 0\n",
    "total_participant = 0\n",
    "wrong_answer_in_pre = dict()\n",
    "for group in df_dict.keys():\n",
    "    for condition in df_dict[group].keys():\n",
    "        _type = group + \"_\" + condition\n",
    "        original = df_dict[group][condition]['scores']\n",
    "        followup = df_dict[group][condition]['scores_followup']\n",
    "        merged_df = pd.merge(original, followup, on=['PROLIFIC_PID', 'q_id'])\n",
    "        for question in ['concept_rst', 'prediction_rst', 'limitation_rst']:\n",
    "            print(\"Question: \", question)\n",
    "            retention_population = merged_df[merged_df[question + \"_x\"] == True]\n",
    "            print(round(len(retention_population[retention_population[question + \"_y\"] == True])/len(retention_population), 4)*100)\n",
    "\n",
    "            # save the data:\n",
    "            if question not in wrong_answer_in_pre.keys():\n",
    "                wrong_answer_in_pre[question] = dict()\n",
    "            retention_population[question + \"_y\"] = retention_population[question + \"_y\"].replace({True: 1, False: 0})\n",
    "            wrong_answer_in_pre[question][_type] = list(retention_population[question + \"_y\"].values)\n",
    "            \n",
    "\n",
    "        total_respondant += len(followup)\n",
    "        total_participant += len(original)\n",
    "        print(\"\\n\")\n",
    "print(\"Total Responding Rate: \", round(total_respondant/total_participant, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Question  concept_rst\n",
      "Chi Squared for  non_native_definition non_native_story\n",
      "12.739406463768152 0.000358031295726987\n",
      "Chi Squared for  native_definition native_story\n",
      "0.0 1.0\n",
      "\n",
      "\n",
      "For Question  prediction_rst\n",
      "Chi Squared for  non_native_definition non_native_story\n",
      "1.591374809949523 0.20712982176873498\n",
      "Chi Squared for  native_definition native_story\n",
      "0.021946010019681522 0.882230816154632\n",
      "\n",
      "\n",
      "For Question  limitation_rst\n",
      "Chi Squared for  non_native_definition non_native_story\n",
      "0.008581283298288127 0.9261933654671165\n",
      "Chi Squared for  native_definition native_story\n",
      "0.0 1.0\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "comparison = [\n",
    "    [\"non_native_definition\", \"non_native_story\"],\n",
    "    [\"native_definition\", \"native_story\"]\n",
    "]\n",
    "for question in ['concept_rst', 'prediction_rst', 'limitation_rst']:\n",
    "    print(\"For Question \", question)\n",
    "    for case1, case2 in comparison:\n",
    "        print(\"Chi Squared for \", case1, case2)\n",
    "        list1 = wrong_answer_in_pre[question][case1]\n",
    "        list2 = wrong_answer_in_pre[question][case2]\n",
    "        table = []\n",
    "        total = 0\n",
    "        for _list in [list1, list2]:  \n",
    "            incorrect = _list.count(0)\n",
    "            correct = _list.count(1)\n",
    "            table.append([correct, incorrect])\n",
    "        stat, p, dof, expected = chi2_contingency(table)\n",
    "        print(stat, p)\n",
    "    print(\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
