{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# libs\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.formula.api as smf\n",
    "import math\n",
    "from scipy.stats import fisher_exact\n",
    "from scipy.stats import chi2_contingency\n",
    "from scipy.stats import mannwhitneyu\n",
    "from tabulate import tabulate\n",
    "import numpy as np\n",
    "from scipy.stats import fisher_exact\n",
    "from tabulate import tabulate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load RCT Results\n",
    "1. Load RCT Result\n",
    "2. Prepare data for accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIR = os.path.join(os.getcwd(), \"rct_results\")\n",
    "\n",
    "df_dict = {}\n",
    "pre_fix = \"10_concepts\"\n",
    "target_questions = ['concept', 'prediction', 'limitation']\n",
    "for group in [\"non_native\", \"native\"]:\n",
    "    if group not in df_dict.keys():\n",
    "        df_dict[group] = dict()\n",
    "    for condition in ['definition', 'story']:\n",
    "        # file_path = os.path.join(INPUT_DIR, '_'.join([pre_fix, group, condition, 'scores.tsv']))\n",
    "        # df = pd.read_csv(file_path, sep=\"\\t\")\n",
    "        # df_dict[group][condition] = df\n",
    "        for study in [\"scores\", \"scores_followup\"]:\n",
    "            file_path = os.path.join(INPUT_DIR, '_'.join([pre_fix, group, condition, study + \".tsv\"]))\n",
    "            study_df = pd.read_csv(file_path, sep='\\t')\n",
    "            df_dict[group][condition][study] = study_df.copy()\n",
    "            # followup_df_dict[group + \"_\" + condition][study] = study_df.copy()\n",
    "\n",
    "for group in df_dict.keys():\n",
    "    for condition in df_dict[group].keys():\n",
    "        for study in [\"scores\", \"scores_followup\"]:\n",
    "            raw_df = df_dict[group][condition][study]\n",
    "            for q in target_questions:\n",
    "                # definition\n",
    "                real_answer = np.array(raw_df[q + \"_question_answer\"].values)\n",
    "                user_answer = np.array(raw_df[q + \"Q\"].values)\n",
    "                correctness = np.multiply((real_answer == user_answer), 1)\n",
    "                raw_df[q + \"_rst\"] = correctness\n",
    "            df_dict[group][condition][study] = raw_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statistical Analysis\n",
    "1. Chi-Squared for the question accuracy in the test and the follow-up test\n",
    "2. Mann-Whitney test for the relevance score and interest in law"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chi Squared for Accuracy\n",
      "For Group  native\n",
      "*************\n",
      "Question:  concept_rst\n",
      "0.48339200428194906 0.4868898821253125\n",
      "*************\n",
      "Question:  prediction_rst\n",
      "0.6537854593213362 0.41876252154968585\n",
      "*************\n",
      "Question:  limitation_rst\n",
      "2.0120481800016 0.1560545129081714\n",
      "For Group  non_native\n",
      "*************\n",
      "Question:  concept_rst\n",
      "0.20182537119577626 0.6532514758125724\n",
      "*************\n",
      "Question:  prediction_rst\n",
      "4.2850363890097 0.0384492595157595\n",
      "*************\n",
      "Question:  limitation_rst\n",
      "11.772340686798307 0.0006011733562701144\n",
      "\n",
      "\n",
      "Mann-Whitney U Test\n",
      "For Group  native\n",
      "Metric:  relevance\n",
      "Story Condition MD(STD):  3.20625 1.3233332677371943\n",
      "Definition Condition MD(STD):  2.6303030303030304 1.3037207473076875\n",
      "16421.0 0.00010137547692223972\n",
      "\n",
      "\n",
      "Metric:  law_interest\n",
      "Story Condition MD(STD):  3.78125 0.9916644782889019\n",
      "Definition Condition MD(STD):  3.6666666666666665 1.119162746219357\n",
      "13800.0 0.45934243459962987\n",
      "\n",
      "\n",
      "For Group  non_native\n",
      "Metric:  relevance\n",
      "Story Condition MD(STD):  3.1882352941176473 1.1681470258697946\n",
      "Definition Condition MD(STD):  2.4702702702702704 1.167317290492911\n",
      "21077.5 1.246935239355139e-08\n",
      "\n",
      "\n",
      "Metric:  law_interest\n",
      "Story Condition MD(STD):  4.029411764705882 1.2001297507707729\n",
      "Definition Condition MD(STD):  3.8378378378378377 0.9157337484376996\n",
      "18662.5 0.0013574027581332746\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Mann Whitney\n",
    "def mannwhitneyu_test(data1, data2):\n",
    "    u1, p = mannwhitneyu(data1, data2, method=\"asymptotic\")\n",
    "    threshold = 0.05\n",
    "    if p < threshold:\n",
    "        # Statistically different\n",
    "        n = True\n",
    "    else:\n",
    "        n = False\n",
    "    return n, u1, p\n",
    "\n",
    "# Chi-Squared test of accuracy\n",
    "print(\"Chi Squared for Accuracy\")\n",
    "for group in [\"native\", \"non_native\"]:\n",
    "    print(\"For Group \", group)\n",
    "    for question in [q + \"_rst\" for q in target_questions]:\n",
    "        print(\"*************\")\n",
    "        print(\"Question: \", question)\n",
    "        contingency_table = []\n",
    "        for condition in [\"story\", \"definition\"]:  \n",
    "            data = df_dict[group][condition]\n",
    "            incorrect = data[question].value_counts()[0]\n",
    "            correct = data[question].value_counts()[1]\n",
    "            contingency_table.append([correct, incorrect])\n",
    "        stat, p, dof, expected = chi2_contingency(contingency_table)\n",
    "        print(stat, p)\n",
    "\n",
    "# Mann-Whitney\n",
    "print(\"\\n\")\n",
    "print(\"Mann-Whitney U Test\")\n",
    "for group in [\"native\", \"non_native\"]:\n",
    "    print(\"For Group \", group)\n",
    "    for metric in [\"relevance\", \"law_interest\"]:  \n",
    "        data1 = df_dict[group]['story'][metric]\n",
    "        data2 = df_dict[group]['definition'][metric]\n",
    "        print(\"Metric: \", metric)\n",
    "        print(\"Story Condition MD(STD): \", np.mean(data1), np.std(data1))\n",
    "        print(\"Definition Condition MD(STD): \", np.mean(data2), np.std(data2))\n",
    "        n, u, p = mannwhitneyu_test(data1, data2)\n",
    "        print(u, p)\n",
    "        print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Followup Assessment result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = os.path.join(os.getcwd(), \"rct_results\")\n",
    "\n",
    "total_participant = []\n",
    "followup_df_dict = {}\n",
    "pre_fix = \"10_concepts\"\n",
    "for group in [\"non_native\", \"native\"]:\n",
    "    for condition in ['definition', 'story']:\n",
    "        if group + \"_\" + condition not in followup_df_dict.keys():\n",
    "            followup_df_dict[group + \"_\" + condition] = dict()\n",
    "        for study in [\"scores\", \"scores_followup\"]:\n",
    "            file_path = os.path.join(DATA_DIR, '_'.join([pre_fix, group, condition, study + \".tsv\"]))\n",
    "            study_df = pd.read_csv(file_path, sep='\\t')\n",
    "            followup_df_dict[group + \"_\" + condition][study] = study_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "non_native_definition\n",
      "Index(['Unnamed: 0_x', 'PROLIFIC_PID', 'q_id', 'law_background', 'age',\n",
      "       'gender', 'edu_background', 'law_interest', 'relevance', 'conceptQ_x',\n",
      "       'predictionQ_x', 'limitationQ_x', 'familiarity_concept_x',\n",
      "       'perceived_difficulty', 'batch_id', 'concept_x',\n",
      "       'concept_question_answer_x', 'prediction_question_answer_x',\n",
      "       'limitation_question_answer_x', 'Unnamed: 0_y', 'conceptQ_y',\n",
      "       'predictionQ_y', 'limitationQ_y', 'familiarity_concept_y', 'concept_y',\n",
      "       'concept_question_answer_y', 'prediction_question_answer_y',\n",
      "       'limitation_question_answer_y'],\n",
      "      dtype='object')\n",
      "Question:  concept_question_answer\n",
      "0.0\n",
      "Question:  prediction_question_answer\n",
      "0.0\n",
      "Question:  limitation_question_answer\n",
      "0.0\n",
      "\n",
      "\n",
      "non_native_story\n",
      "Index(['Unnamed: 0_x', 'PROLIFIC_PID', 'q_id', 'law_background', 'age',\n",
      "       'gender', 'edu_background', 'law_interest', 'relevance', 'conceptQ_x',\n",
      "       'predictionQ_x', 'limitationQ_x', 'familiarity_concept_x',\n",
      "       'perceived_difficulty', 'familiarity_story_setting', 'batch_id',\n",
      "       'concept_x', 'concept_question_answer_x',\n",
      "       'prediction_question_answer_x', 'limitation_question_answer_x',\n",
      "       'Unnamed: 0_y', 'conceptQ_y', 'predictionQ_y', 'limitationQ_y',\n",
      "       'familiarity_concept_y', 'concept_y', 'concept_question_answer_y',\n",
      "       'prediction_question_answer_y', 'limitation_question_answer_y'],\n",
      "      dtype='object')\n",
      "Question:  concept_question_answer\n",
      "0.0\n",
      "Question:  prediction_question_answer\n",
      "0.0\n",
      "Question:  limitation_question_answer\n",
      "0.0\n",
      "\n",
      "\n",
      "native_definition\n",
      "Index(['Unnamed: 0_x', 'PROLIFIC_PID', 'q_id', 'law_background', 'age',\n",
      "       'gender', 'edu_background', 'law_interest', 'relevance', 'conceptQ_x',\n",
      "       'predictionQ_x', 'limitationQ_x', 'familiarity_concept_x',\n",
      "       'perceived_difficulty', 'batch_id', 'concept_x',\n",
      "       'concept_question_answer_x', 'prediction_question_answer_x',\n",
      "       'limitation_question_answer_x', 'Unnamed: 0_y', 'conceptQ_y',\n",
      "       'predictionQ_y', 'limitationQ_y', 'familiarity_concept_y', 'concept_y',\n",
      "       'concept_question_answer_y', 'prediction_question_answer_y',\n",
      "       'limitation_question_answer_y'],\n",
      "      dtype='object')\n",
      "Question:  concept_question_answer\n",
      "0.0\n",
      "Question:  prediction_question_answer\n",
      "0.0\n",
      "Question:  limitation_question_answer\n",
      "0.0\n",
      "\n",
      "\n",
      "native_story\n",
      "Index(['Unnamed: 0_x', 'PROLIFIC_PID', 'q_id', 'law_background', 'age',\n",
      "       'gender', 'edu_background', 'law_interest', 'relevance', 'conceptQ_x',\n",
      "       'predictionQ_x', 'limitationQ_x', 'familiarity_concept_x',\n",
      "       'perceived_difficulty', 'familiarity_story_setting', 'batch_id',\n",
      "       'concept_x', 'concept_question_answer_x',\n",
      "       'prediction_question_answer_x', 'limitation_question_answer_x',\n",
      "       'Unnamed: 0_y', 'conceptQ_y', 'predictionQ_y', 'limitationQ_y',\n",
      "       'familiarity_concept_y', 'concept_y', 'concept_question_answer_y',\n",
      "       'prediction_question_answer_y', 'limitation_question_answer_y'],\n",
      "      dtype='object')\n",
      "Question:  concept_question_answer\n",
      "0.0\n",
      "Question:  prediction_question_answer\n",
      "0.0\n",
      "Question:  limitation_question_answer\n",
      "0.0\n",
      "\n",
      "\n",
      "Total Responding Rate:  0.71\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/zl/b_kmp9gj7nx2fx9721_dt5mc0000gn/T/ipykernel_13318/2995976724.py:23: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  retention_population[question + \"_y\"] = retention_population[question + \"_y\"].replace({True: 1, False: 0})\n",
      "/var/folders/zl/b_kmp9gj7nx2fx9721_dt5mc0000gn/T/ipykernel_13318/2995976724.py:23: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  retention_population[question + \"_y\"] = retention_population[question + \"_y\"].replace({True: 1, False: 0})\n",
      "/var/folders/zl/b_kmp9gj7nx2fx9721_dt5mc0000gn/T/ipykernel_13318/2995976724.py:23: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  retention_population[question + \"_y\"] = retention_population[question + \"_y\"].replace({True: 1, False: 0})\n",
      "/var/folders/zl/b_kmp9gj7nx2fx9721_dt5mc0000gn/T/ipykernel_13318/2995976724.py:23: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  retention_population[question + \"_y\"] = retention_population[question + \"_y\"].replace({True: 1, False: 0})\n"
     ]
    }
   ],
   "source": [
    "total_respondant = 0\n",
    "total_participant = 0\n",
    "focused_population_result = dict()\n",
    "for _type in followup_df_dict.keys():\n",
    "    # old_file = os.path.join(os.getcwd(), \"rct_exp\", '_'.join([pre_fix, _type, 'scores.csv']))\n",
    "    # old_df = pd.read_csv(old_file)\n",
    "    print(_type)\n",
    "    # print(\"Reply Rate: \", round(len(followup)/len(old_df), 2))\n",
    "\n",
    "    original = followup_df_dict[_type]['scores']\n",
    "    followup = followup_df_dict[_type]['scores_followup']\n",
    "    merged_df = pd.merge(original, followup, on=['PROLIFIC_PID', 'q_id'])\n",
    "    print(merged_df.columns)\n",
    "    # for question in ['concept', 'q2_pred', 'q3_pred']:\n",
    "    for question in ['concept_question_answer', 'prediction_question_answer', 'limitation_question_answer']:\n",
    "        print(\"Question: \", question)\n",
    "        retention_population = merged_df[merged_df[question + \"_x\"] == True]\n",
    "        print(round(len(retention_population[retention_population[question + \"_y\"] == True])/len(retention_population), 4)*100)\n",
    "\n",
    "        # save the data:\n",
    "        if question not in focused_population_result.keys():\n",
    "            focused_population_result[question] = dict()\n",
    "        retention_population[question + \"_y\"] = retention_population[question + \"_y\"].replace({True: 1, False: 0})\n",
    "        focused_population_result[question][_type] = list(retention_population[question + \"_y\"].values)\n",
    "        \n",
    "\n",
    "    total_respondant += len(followup)\n",
    "    total_participant += len(original)\n",
    "    print(\"\\n\")\n",
    "print(\"Total Responding Rate: \", round(total_respondant/total_participant, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "q1_pred\n",
      "non_native_definition non_native_story\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'q1_pred'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m case1, case2 \u001b[38;5;129;01min\u001b[39;00m comparison:\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28mprint\u001b[39m(case1, case2)\n\u001b[0;32m----> 9\u001b[0m     list1 \u001b[38;5;241m=\u001b[39m \u001b[43mfocused_population_result\u001b[49m\u001b[43m[\u001b[49m\u001b[43mquestion\u001b[49m\u001b[43m]\u001b[49m[case1]\n\u001b[1;32m     10\u001b[0m     list2 \u001b[38;5;241m=\u001b[39m focused_population_result[question][case2]\n\u001b[1;32m     11\u001b[0m     table \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[0;31mKeyError\u001b[0m: 'q1_pred'"
     ]
    }
   ],
   "source": [
    "comparison = [\n",
    "    [\"non_native_definition\", \"non_native_story\"],\n",
    "    [\"native_definition\", \"native_story\"]\n",
    "]\n",
    "for question in ['q1_pred', 'q2_pred', 'q3_pred']:\n",
    "    print(question)\n",
    "    for case1, case2 in comparison:\n",
    "        print(case1, case2)\n",
    "        list1 = focused_population_result[question][case1]\n",
    "        list2 = focused_population_result[question][case2]\n",
    "        table = []\n",
    "        total = 0\n",
    "        for _list in [list1, list2]:  \n",
    "            incorrect = _list.count(0)\n",
    "            correct = _list.count(1)\n",
    "            table.append([correct, incorrect])\n",
    "            print(correct/(correct+incorrect))\n",
    "            total += (correct + incorrect)\n",
    "        # print(tabulate(table))\n",
    "        print(total)\n",
    "        print(\"Chi Squared\")\n",
    "        stat, p, dof, expected = chi2_contingency(table)\n",
    "        print(stat, p)\n",
    "    print(\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
