{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Human Evaluation Scores\n",
    "- stories: readability, relevance, redundancy, cohesiveness, co pleteness, factuality, likeability, believability\n",
    "- questions: q1-rating (concept question), q2-rating (ending question), q3-rating (limitation question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def get_concat_scores(df_current_batch, q_ids, is_batch_20=False):\n",
    "\n",
    "    question_types = ['readability-1', 'readability-2', 'relevance', 'redundancy', 'cohesiveness', 'completeness', 'factuality', 'likeability', 'believability', 'q1-rating', 'q2-rating', 'q3-rating']\n",
    "    df_list = []\n",
    "    end_batch_num = 88 if is_batch_20 else 86 # the 20th batch contains 7 concepts instead of 5\n",
    "    for q_num, q_id in zip(range(81, end_batch_num), q_ids):\n",
    "        mean_row, std_row = [], []\n",
    "        questions_per_instance = []\n",
    "        for q_type in question_types:\n",
    "            q_full_name = \"Q{}-{}\".format(q_num, q_type)\n",
    "            questions_per_instance.append(q_full_name)\n",
    "        df_ = pd.DataFrame(df_current_batch[questions_per_instance].astype(float).values, columns=question_types)\n",
    "        df_['q_id'] = q_id\n",
    "            # print(df_.mean(), df_.std())\n",
    "        df_list.append(df_)\n",
    "\n",
    "    df_concat = pd.concat(df_list)\n",
    "    return df_concat\n",
    "\n",
    "def concatenate_batches(model_name, num_batches):\n",
    "\n",
    "    df_llm_batches = []\n",
    "    for batch_id in range(1, num_batches+1):\n",
    "        df_batch = pd.read_csv(\"./prolific_annotations/{}/Legal-Story-Batch-{}.csv\".format(model_name, batch_id)).iloc[2:5]\n",
    "        if batch_id == 20:\n",
    "            q_ids = list(range(5*batch_id-5, 5*batch_id+2))\n",
    "            df_concat = get_concat_scores(df_batch, q_ids, is_batch_20=True)\n",
    "        else:\n",
    "            q_ids = list(range(5*batch_id-5, 5*batch_id))\n",
    "            df_concat = get_concat_scores(df_batch,  q_ids)\n",
    "        df_llm_batches.append(df_concat)\n",
    "\n",
    "    df_concat_batches = pd.concat(df_llm_batches)\n",
    "    return df_concat_batches"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPT-4 (101 concepts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(303, 13)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_concat_batches_gpt4 = concatenate_batches('gpt4', 20)\n",
    "\n",
    "# remove q_id=42 because this is a duplicate\n",
    "df_concat_batches_gpt4 = df_concat_batches_gpt4[df_concat_batches_gpt4.q_id != 42]\n",
    "df_concat_batches_gpt4.q_id = df_concat_batches_gpt4.q_id.apply(lambda x: x-1 if x > 42 else x)\n",
    "df_concat_batches_gpt4.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(303, 20)\n",
      "(101, 20)\n"
     ]
    }
   ],
   "source": [
    "df_101 = pd.read_csv(\"../data/101-doctrines/gpt4_story_question_101.tsv\", delimiter=\"\\t\")\n",
    "df_101['q_id'] = df_101.index\n",
    "\n",
    "# save all annotations\n",
    "df_101_merge = df_concat_batches_gpt4.merge(df_101, on=\"q_id\")\n",
    "df_101_merge.to_csv(\"../analysis/prolific_annotations/gpt4/101_raw_annotation_scores.tsv\", sep=\"\\t\")\n",
    "print(df_101_merge.shape)\n",
    "\n",
    "# save mean scores\n",
    "df_concat_batches_mean = df_concat_batches_gpt4.groupby(by=\"q_id\").mean().round(2).reset_index()\n",
    "df_concat_batches_mean_merge = df_concat_batches_mean.merge(df_101, on=\"q_id\")\n",
    "df_concat_batches_mean_merge.to_csv(\"../analysis/prolific_annotations/gpt4/101_mean_annotation_scores.tsv\", sep=\"\\t\")\n",
    "print(df_concat_batches_mean_merge.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(303, 13)\n",
      "mean scores [3.95, 4.66, 4.56, 4.0, 4.63, 4.57, 4.56, 4.36, 4.54, 4.46, 4.35, 4.14, 50.0]\n",
      "std scores [1.04, 0.6, 0.71, 1.26, 0.62, 0.67, 0.69, 0.81, 0.74, 0.77, 0.78, 0.98, 29.2]\n"
     ]
    }
   ],
   "source": [
    "df_concat_batches_gpt4 = df_concat_batches_gpt4[['readability-1', 'readability-2', 'relevance', 'redundancy',\n",
    "                                                 'cohesiveness', 'completeness', 'factuality', 'likeability',\n",
    "                                                 'believability', 'q1-rating', 'q2-rating', 'q3-rating', 'q_id']]\n",
    "print(df_concat_batches_gpt4.shape)\n",
    "\n",
    "print(\"mean scores\", df_concat_batches_gpt4.mean().round(2).values.tolist())\n",
    "print(\"std scores\", df_concat_batches_gpt4.std().round(2).values.tolist())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPT-4 (20 concepts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>concept</th>\n",
       "      <th>q_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Abstention_doctrine</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Brady_disclosure</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Commanding_precedent</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Constitutional_convention_(political_custom)</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Doctrine_of_foreign_equivalents</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>FTC_v._Dean_Foods_Co.</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Learned_intermediary</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Maxwellisation</td>\n",
       "      <td>54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Pacta_sunt_servanda</td>\n",
       "      <td>65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Plain_meaning_rule</td>\n",
       "      <td>69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Presumption_(Catholic_canon_law)</td>\n",
       "      <td>72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Prosecutorial_discretion</td>\n",
       "      <td>77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Purcell_principle</td>\n",
       "      <td>78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Remoteness_in_English_law</td>\n",
       "      <td>82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Safe_harbor_(law)</td>\n",
       "      <td>86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Substantial_certainty_doctrine</td>\n",
       "      <td>89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Unaccompanied_minor</td>\n",
       "      <td>93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Undue_hardship</td>\n",
       "      <td>95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Volenti_non_fit_injuria</td>\n",
       "      <td>99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Wrongdoing</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         concept  q_id\n",
       "0                            Abstention_doctrine     0\n",
       "1                               Brady_disclosure     7\n",
       "2                           Commanding_precedent     9\n",
       "3   Constitutional_convention_(political_custom)    11\n",
       "4                Doctrine_of_foreign_equivalents    18\n",
       "5                          FTC_v._Dean_Foods_Co.    35\n",
       "6                           Learned_intermediary    49\n",
       "7                                 Maxwellisation    54\n",
       "8                            Pacta_sunt_servanda    65\n",
       "9                             Plain_meaning_rule    69\n",
       "10              Presumption_(Catholic_canon_law)    72\n",
       "11                      Prosecutorial_discretion    77\n",
       "12                             Purcell_principle    78\n",
       "13                     Remoteness_in_English_law    82\n",
       "14                             Safe_harbor_(law)    86\n",
       "15                Substantial_certainty_doctrine    89\n",
       "16                           Unaccompanied_minor    93\n",
       "17                                Undue_hardship    95\n",
       "18                       Volenti_non_fit_injuria    99\n",
       "19                                    Wrongdoing   100"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_20 = pd.read_csv(\"../data/20-doctrines/legal_doctrines_20.tsv\", delimiter=\"\\t\")\n",
    "df_101 = pd.read_csv(\"../data/101-doctrines/gpt4_story_question_101.tsv\", delimiter=\"\\t\")\n",
    "df_101['q_id'] = df_101.index\n",
    "\n",
    "df_sampled_20_name_idx = df_101.merge(df_20, on=\"concept\")[['concept', 'q_id']]\n",
    "df_sampled_20_name_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60, 14)\n",
      "mean scores: [3.98, 4.7, 4.52, 3.78, 4.57, 4.58, 4.52, 4.42, 4.48, 4.42, 4.37, 4.15, 59.4]\n",
      "std scores [1.07, 0.46, 0.68, 1.29, 0.56, 0.53, 0.62, 0.79, 0.7, 0.72, 0.8, 1.04, 33.63]\n"
     ]
    }
   ],
   "source": [
    "df_sampled_scores = df_concat_batches_gpt4.merge(df_sampled_20_name_idx, on=\"q_id\")\n",
    "print(df_sampled_scores.shape)\n",
    "df_sampled_scores = df_sampled_scores[['readability-1', 'readability-2', 'relevance', 'redundancy', 'cohesiveness', 'completeness', 'factuality', 'likeability', 'believability', 'q1-rating', 'q2-rating', 'q3-rating', 'q_id']]\n",
    "print(\"mean scores:\", df_sampled_scores.mean().round(2).values.tolist())\n",
    "print(\"std scores\", df_sampled_scores.std().round(2).values.tolist())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPT-3.5 (20 concepts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60, 13)\n",
      "mean scores [3.3, 4.35, 4.2, 3.72, 4.3, 4.03, 4.12, 4.1, 4.13, 4.12, 3.95, 3.48, 9.5]\n",
      "std scores [1.01, 0.68, 0.78, 0.8, 0.74, 0.78, 0.69, 0.95, 0.65, 0.64, 0.85, 0.91, 5.81]\n"
     ]
    }
   ],
   "source": [
    "df_concat_batches_chatgpt = concatenate_batches('gpt3.5', 4)\n",
    "print(df_concat_batches_chatgpt.shape)\n",
    "df_concat_batches_chatgpt.head(5)\n",
    "\n",
    "# fix the q_ids here\n",
    "df_20_concept = pd.read_csv(\"../data/20-doctrines/gpt3.5_story_question_20.tsv\", delimiter=\"\\t\")[['concept']]\n",
    "df_20_concept['q_id'] = df_20_concept.index\n",
    "df_concat_batches_chatgpt = df_concat_batches_chatgpt.merge(df_20_concept, on=\"q_id\")\n",
    "\n",
    "df_concat_batches_chatgpt = df_concat_batches_chatgpt[['readability-1', 'readability-2', 'relevance', 'redundancy', 'cohesiveness', 'completeness', 'factuality', 'likeability', 'believability', 'q1-rating', 'q2-rating', 'q3-rating', 'q_id']]\n",
    "print(\"mean scores\", df_concat_batches_chatgpt.mean().round(2).values.tolist())\n",
    "print(\"std scores\", df_concat_batches_chatgpt.std().round(2).values.tolist())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLaMA 2 (20 concepts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>concept</th>\n",
       "      <th>q_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Abstention_doctrine</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Brady_disclosure</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Commanding_precedent</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Constitutional_convention_(political_custom)</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Doctrine_of_foreign_equivalents</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>FTC_v._Dean_Foods_Co.</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Learned_intermediary</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Maxwellisation</td>\n",
       "      <td>54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Pacta_sunt_servanda</td>\n",
       "      <td>65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Plain_meaning_rule</td>\n",
       "      <td>69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Presumption_(Catholic_canon_law)</td>\n",
       "      <td>72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Prosecutorial_discretion</td>\n",
       "      <td>77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Purcell_principle</td>\n",
       "      <td>78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Remoteness_in_English_law</td>\n",
       "      <td>82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Safe_harbor_(law)</td>\n",
       "      <td>86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Substantial_certainty_doctrine</td>\n",
       "      <td>89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Unaccompanied_minor</td>\n",
       "      <td>93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Undue_hardship</td>\n",
       "      <td>95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Volenti_non_fit_injuria</td>\n",
       "      <td>99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Wrongdoing</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         concept  q_id\n",
       "0                            Abstention_doctrine     0\n",
       "1                               Brady_disclosure     7\n",
       "2                           Commanding_precedent     9\n",
       "3   Constitutional_convention_(political_custom)    11\n",
       "4                Doctrine_of_foreign_equivalents    18\n",
       "5                          FTC_v._Dean_Foods_Co.    35\n",
       "6                           Learned_intermediary    49\n",
       "7                                 Maxwellisation    54\n",
       "8                            Pacta_sunt_servanda    65\n",
       "9                             Plain_meaning_rule    69\n",
       "10              Presumption_(Catholic_canon_law)    72\n",
       "11                      Prosecutorial_discretion    77\n",
       "12                             Purcell_principle    78\n",
       "13                     Remoteness_in_English_law    82\n",
       "14                             Safe_harbor_(law)    86\n",
       "15                Substantial_certainty_doctrine    89\n",
       "16                           Unaccompanied_minor    93\n",
       "17                                Undue_hardship    95\n",
       "18                       Volenti_non_fit_injuria    99\n",
       "19                                    Wrongdoing   100"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_20 = pd.read_csv(\"../data/20-doctrines/llama2_story_question_20.tsv\", delimiter=\"\\t\")\n",
    "df_101 = pd.read_csv(\"../data/101-doctrines/llama2_story_question_101.tsv\", delimiter=\"\\t\")\n",
    "df_101['q_id'] = df_101.index\n",
    "df_sampled_20_name_idx = df_101.merge(df_20, on=\"concept\")[['concept', 'q_id']]\n",
    "df_sampled_20_name_idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60, 13)\n",
      "(60, 14)\n",
      "mean scores [3.72, 4.35, 4.4, 3.92, 4.38, 4.15, 4.1, 4.2, 4.35, 4.23, 4.1, 4.12, 9.5]\n",
      "std scores [1.15, 0.86, 0.85, 1.33, 0.83, 1.12, 1.17, 1.04, 0.94, 0.96, 1.22, 1.12, 5.81]\n"
     ]
    }
   ],
   "source": [
    "df_concat_batches_llama2 = concatenate_batches('llama2', 4)\n",
    "print(df_concat_batches_llama2.shape)\n",
    "df_concat_batches_llama2.head(5)\n",
    "\n",
    "# fix the q_ids here\n",
    "df_20_concept = pd.read_csv(\"../data/20-doctrines/llama2_story_question_20.tsv\", delimiter=\"\\t\")[['concept']]\n",
    "df_20_concept['q_id'] = df_20_concept.index\n",
    "df_concat_batches_llama2 = df_concat_batches_llama2.merge(df_20_concept, on=\"q_id\")\n",
    "print(df_concat_batches_llama2.shape)\n",
    "\n",
    "df_concat_batches_llama2 = df_concat_batches_llama2[['readability-1', 'readability-2', 'relevance', 'redundancy', 'cohesiveness', 'completeness', 'factuality', 'likeability', 'believability', 'q1-rating', 'q2-rating', 'q3-rating', 'q_id']]\n",
    "print(\"mean scores\", df_concat_batches_llama2.mean().round(2).values.tolist())\n",
    "print(\"std scores\", df_concat_batches_llama2.std().round(2).values.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error Type Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_concat_comments(df_current_batch, q_ids, is_batch_20=False):\n",
    "\n",
    "    question_types = ['story-comments', 'q1-issues', 'q2-issues', 'q3-issues']\n",
    "    df_list = []\n",
    "    end_batch_num = 88 if is_batch_20 else 86 # the 20th batch contains 7 concepts instead of 5\n",
    "    for q_num, q_id in zip(range(81, end_batch_num), q_ids):\n",
    "        questions_per_instance = []\n",
    "        for q_type in question_types:\n",
    "            q_full_name = \"Q{}-{}\".format(q_num, q_type)\n",
    "            questions_per_instance.append(q_full_name)\n",
    "        df_ = pd.DataFrame(df_current_batch[questions_per_instance].astype(str).values, columns=question_types)\n",
    "        df_['q_id'] = q_id\n",
    "            # print(df_.mean(), df_.std())\n",
    "        df_list.append(df_)\n",
    "\n",
    "    df_concat = pd.concat(df_list)\n",
    "    return df_concat\n",
    "\n",
    "def concatenate_batches_comments(model_name, num_batches):\n",
    "\n",
    "    df_llm_batches = []\n",
    "    for batch_id in range(1, num_batches+1):\n",
    "        df_batch = pd.read_csv(\"./prolific_annotations/{}/Legal-Story-Batch-{}.csv\".format(model_name, batch_id)).iloc[2:5]\n",
    "        if batch_id == 20:\n",
    "            q_ids = list(range(5*batch_id-5, 5*batch_id+2))\n",
    "            df_concat = get_concat_comments(df_batch, q_ids, is_batch_20=True)\n",
    "        else:\n",
    "            q_ids = list(range(5*batch_id-5, 5*batch_id))\n",
    "            df_concat = get_concat_comments(df_batch,  q_ids)\n",
    "        df_llm_batches.append(df_concat)\n",
    "\n",
    "    df_concat_batches = pd.concat(df_llm_batches)\n",
    "    return df_concat_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_comment_statistics(comment_list):\n",
    "    number_instances = len(comment_list)\n",
    "    error_type_dict = {}\n",
    "    for each_comment_list in comment_list:\n",
    "        for each_comment in each_comment_list.split(\",\"):\n",
    "            if each_comment in error_type_dict:\n",
    "                error_type_dict[each_comment] += 1\n",
    "            else:\n",
    "                error_type_dict[each_comment] = 1\n",
    "    \n",
    "    return {k:round(100*float(v)/number_instances, 2) for k, v in error_type_dict.items()}\n",
    "\n",
    "def get_no_issue_statistics(comment_list):\n",
    "    number_instances = len(comment_list)\n",
    "    error_type_dict = {}\n",
    "    for each_comment_list in comment_list:\n",
    "        if each_comment_list == \"There is no issue.\":\n",
    "            if each_comment_list in error_type_dict:\n",
    "                error_type_dict[each_comment_list] += 1\n",
    "            else:\n",
    "                error_type_dict[each_comment_list] = 1\n",
    "    \n",
    "    return {k:round(100*float(v)/number_instances, 2) for k, v in error_type_dict.items()}\n",
    "\n",
    "\n",
    "def get_error_type_rate(df_agg_comments, error_types):\n",
    "    q1_error_rates = get_no_issue_statistics(df_agg_comments['q1-issues'].tolist())\n",
    "\n",
    "    q2_error_rates = get_no_issue_statistics(df_agg_comments['q2-issues'].tolist())\n",
    "\n",
    "    q3_error_rates = get_no_issue_statistics(df_agg_comments['q3-issues'].tolist())\n",
    "\n",
    "    rows = []\n",
    "    for error_type in error_types:\n",
    "        row = []\n",
    "        for current_dict in [q1_error_rates, q2_error_rates, q3_error_rates]:\n",
    "            if error_type in current_dict:\n",
    "                row.append(current_dict[error_type])\n",
    "            else:\n",
    "                row.append(0.0)\n",
    "        rows.append(row)\n",
    "    df_rates = pd.DataFrame(rows, columns=['concept_question', 'prediction_question', 'limitation_question'], index=error_types)\n",
    "    return df_rates.sort_index()\n",
    "\n",
    "# ensure all the 8 error types are covered in the 101 examples\n",
    "# error_types = sorted(get_comment_statistics(df_concat_gpt4_comments['q1-issues'].tolist()))\n",
    "# get_error_type_rate(df_concat_gpt4_comments, error_types)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPT-4 No Issue Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(303, 5)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# need to remove one duplicate for gpt-4 annotations \n",
    "df_concat_gpt4_comments = concatenate_batches_comments('gpt4', 20)\n",
    "df_concat_gpt4_comments = df_concat_gpt4_comments[df_concat_gpt4_comments.q_id != 42]\n",
    "df_concat_gpt4_comments.q_id = df_concat_gpt4_comments.q_id.apply(lambda x: x-1 if x > 42 else x)\n",
    "df_concat_gpt4_comments.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60, 6)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>concept_question</th>\n",
       "      <th>prediction_question</th>\n",
       "      <th>limitation_question</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>There is no issue.</th>\n",
       "      <td>83.33</td>\n",
       "      <td>75.0</td>\n",
       "      <td>80.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    concept_question  prediction_question  limitation_question\n",
       "There is no issue.             83.33                 75.0                 80.0"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sampled_gpt4_20_comments = df_concat_gpt4_comments.merge(df_sampled_20_name_idx, on=\"q_id\")\n",
    "df_sampled_gpt4_20_comments = df_sampled_gpt4_20_comments[['concept', 'q_id', 'story-comments', 'q1-issues', 'q2-issues', 'q3-issues']]\n",
    "df_sampled_gpt4_20_comments.to_csv(\"./prolific_annotations/gpt4/gpt4_sampled_20_errors.tsv\", sep=\"\\t\")\n",
    "print(df_sampled_gpt4_20_comments.shape)\n",
    "\n",
    "error_types = sorted(get_comment_statistics(df_concat_gpt4_comments['q1-issues'].tolist()))\n",
    "# only counts if an annotator marks it \"There is no issue.\" and marks no other issue at the same time\n",
    "get_error_type_rate(df_sampled_gpt4_20_comments, [\"There is no issue.\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPT-3.5 No Issue Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60, 6)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>concept_question</th>\n",
       "      <th>prediction_question</th>\n",
       "      <th>limitation_question</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>There is no issue.</th>\n",
       "      <td>71.67</td>\n",
       "      <td>71.67</td>\n",
       "      <td>41.67</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    concept_question  prediction_question  limitation_question\n",
       "There is no issue.             71.67                71.67                41.67"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_concat_chatgpt_comments = concatenate_batches_comments('gpt3.5', 4)\n",
    "\n",
    "df_concat_chatgpt_comments = df_concat_chatgpt_comments.merge(df_20_concept, on=\"q_id\")\n",
    "df_concat_chatgpt_comments = df_concat_chatgpt_comments[['concept', 'q_id', 'story-comments', 'q1-issues', 'q2-issues', 'q3-issues']]\n",
    "df_concat_chatgpt_comments.to_csv(\"./prolific_annotations/gpt3.5/gpt3.5_sampled_20_errors.tsv\", sep=\"\\t\")\n",
    "print(df_concat_chatgpt_comments.shape)\n",
    "\n",
    "error_types = sorted(get_comment_statistics(df_concat_gpt4_comments['q1-issues'].tolist()))\n",
    "get_error_type_rate(df_concat_chatgpt_comments, [\"There is no issue.\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLaMA 2 No Issue Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60, 6)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>concept_question</th>\n",
       "      <th>prediction_question</th>\n",
       "      <th>limitation_question</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>There is no issue.</th>\n",
       "      <td>66.67</td>\n",
       "      <td>60.0</td>\n",
       "      <td>65.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    concept_question  prediction_question  limitation_question\n",
       "There is no issue.             66.67                 60.0                 65.0"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_concat_llama2_comments = concatenate_batches_comments('llama2', 4)\n",
    "df_concat_llama2_comments = df_concat_llama2_comments.merge(df_20_concept, on=\"q_id\")\n",
    "df_concat_llama2_comments = df_concat_llama2_comments[['concept', 'q_id', 'story-comments', 'q1-issues', 'q2-issues', 'q3-issues']]\n",
    "df_concat_llama2_comments.to_csv(\"./prolific_annotations/llama2/llama2_sampled_20_errors.tsv\", sep=\"\\t\")\n",
    "print(df_concat_llama2_comments.shape)\n",
    "\n",
    "error_types = sorted(get_comment_statistics(df_concat_llama2_comments['q1-issues'].tolist()))\n",
    "# only counts if an annotator marks it \"There is no issue.\" and marks no other issue at the same time\n",
    "get_error_type_rate(df_concat_llama2_comments, [\"There is no issue.\"])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "audiencenlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
